{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_HW03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8COawULBVgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from torch.nn import functional as F\n",
        "\n",
        "SEED = 423  # 627, 8, 11\n",
        "\n",
        "\n",
        "def make_reproducible(seed, make_cuda_reproducible):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if make_cuda_reproducible:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "make_reproducible(SEED, make_cuda_reproducible=False)\n",
        "\n",
        "\n",
        "def transform_state(state):\n",
        "    return torch.tensor(state).float().to(DEVICE)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, hidden=400, in_dim=3, out_dim=1):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_dim, hidden)\n",
        "        self.mu = nn.Linear(hidden, out_dim)\n",
        "        self.sigma = nn.Parameter(torch.full((1,), np.log(0.6)), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z2 = F.relu(self.fc(x))\n",
        "        mu = 2 * F.tanh(self.mu(z2))\n",
        "        sigma = self.sigma.expand_as(mu).exp()\n",
        "        return mu, sigma\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.actor = Agent.generate_model()\n",
        "        self.actor.load_state_dict(torch.load(__file__[:-8] + \"/agent.pkl\"))\n",
        "        self.actor.to(torch.device(\"cpu\"))\n",
        "        self.actor.eval()\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_model():\n",
        "        return Actor()\n",
        "\n",
        "    def act(self, state):\n",
        "        state = transform_state(state)\n",
        "        out = self.actor(state)\n",
        "        return np.array([Normal(out[0], out[1]).sample().item()])\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Eb_Z9XaBXmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "\n",
        "from gym import make\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from torch.optim import Adam\n",
        "\n",
        "N_STEP = 1\n",
        "GAMMA = 0.9\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "UPDATE_LENGTH = 10\n",
        "ENTROPY = 0.01\n",
        "TARGET_UPDATE = 800\n",
        "EPOSIODE_LEN = 200\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, hidden=400, in_dim=3, out_dim=1):\n",
        "        super().__init__()\n",
        "        self.fc2 = nn.Linear(in_dim, hidden)\n",
        "        self.valNet = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = F.relu(self.fc2(x))\n",
        "        val = self.valNet(z)\n",
        "        return val\n",
        "\n",
        "\n",
        "class A2C:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.gamma = GAMMA ** N_STEP\n",
        "        self.actor = Agent.generate_model().to(DEVICE)  # Torch model\n",
        "        self.critic = Critic().to(DEVICE)  # Torch model\n",
        "        self.actor_optimizer = Adam(self.actor.parameters(), lr=0.0001)\n",
        "        self.critic_optimizer = Adam(self.critic.parameters(), lr=0.001)\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.entropies = []\n",
        "        self.critic_values = []\n",
        "        self.target_values = []\n",
        "        self.update_steps = 0\n",
        "        self.distribution = None\n",
        "        self.target = deepcopy(self.critic)\n",
        "\n",
        "    def optimizer_step(self, log_probs, entropies, returns, critic_values):\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        adv = (returns - critic_values).detach()\n",
        "        policy_loss = -(log_probs * adv).mean()\n",
        "        entropy_loss = -entropies.mean() * ENTROPY\n",
        "        value_loss = ((critic_values - returns) ** 2 / 2).mean()\n",
        "        total_loss = value_loss + entropy_loss + policy_loss\n",
        "        total_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.update_steps += 1\n",
        "        if self.update_steps % TARGET_UPDATE == 0:\n",
        "            self.target = deepcopy(self.critic)\n",
        "\n",
        "        state, action, next_state, reward, done = transition\n",
        "        action = torch.tensor(action)\n",
        "        self.states.append(transform_state(state))\n",
        "        self.actions.append(action)\n",
        "        self.next_states.append(transform_state(next_state))\n",
        "        self.rewards.append((reward + 8.1) / 8.1)\n",
        "        self.log_probs.append(self.distribution.log_prob(action))\n",
        "        self.entropies.append(self.distribution.entropy())\n",
        "        self.critic_values.append(self.critic(transform_state(state)))\n",
        "        self.target_values.append(self.target(transform_state(state)))\n",
        "        if done or len(self.states) == UPDATE_LENGTH:\n",
        "            next_target = torch.zeros(1) if done else self.target(self.next_states[-1])\n",
        "            if len(self.target_values) > 1:\n",
        "                returns = torch.tensor(self.rewards).view(-1, 1) + \\\n",
        "                          self.gamma * torch.cat((torch.cat(self.target_values[1:]), next_target)).view(-1, 1).detach()\n",
        "            else:\n",
        "                self.states = []\n",
        "                self.actions = []\n",
        "                self.next_states = []\n",
        "                self.rewards = []\n",
        "                self.log_probs = []\n",
        "                self.entropies = []\n",
        "                self.critic_values = []\n",
        "                self.target_values = []\n",
        "                return\n",
        "\n",
        "            self.optimizer_step(\n",
        "                torch.cat(self.log_probs).view(-1, 1),\n",
        "                torch.cat(self.entropies).view(-1, 1),\n",
        "                returns,\n",
        "                torch.cat(self.critic_values).view(-1, 1)\n",
        "            )\n",
        "\n",
        "            self.states = []\n",
        "            self.actions = []\n",
        "            self.next_states = []\n",
        "            self.rewards = []\n",
        "            self.log_probs = []\n",
        "            self.entropies = []\n",
        "            self.critic_values = []\n",
        "            self.target_values = []\n",
        "\n",
        "    def act(self, state):\n",
        "        # Remember: agent is not deterministic, sample actions from distribution (e.g. Gaussian)\n",
        "        state = transform_state(state)\n",
        "        out = self.actor(state)\n",
        "        self.distribution = Normal(out[0], out[1])\n",
        "        return np.array([self.distribution.sample().item()])\n",
        "\n",
        "    def save(self, i):\n",
        "        torch.save(self.actor.state_dict(), f'agent_{i}.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnRWCN3wBejp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "402e11c6-4a86-49ea-9727-9c110da9b5ee"
      },
      "source": [
        "env = make(\"Pendulum-v0\")\n",
        "a2c = A2C(state_dim=3, action_dim=1)\n",
        "episodes = 10000\n",
        "\n",
        "scores = []\n",
        "best_score = -10000.0\n",
        "best_score_25 = -10000.0\n",
        "total_steps = 0\n",
        "start = time.time()\n",
        "\n",
        "for i in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    reward_buffer = deque(maxlen=N_STEP)\n",
        "    state_buffer = deque(maxlen=N_STEP)\n",
        "    action_buffer = deque(maxlen=N_STEP)\n",
        "    while not done:\n",
        "        if steps == EPOSIODE_LEN:\n",
        "            break\n",
        "        total_steps += 1\n",
        "        action = a2c.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        reward_buffer.append(reward)\n",
        "        state_buffer.append(state)\n",
        "        action_buffer.append(action)\n",
        "        if len(reward_buffer) == N_STEP:\n",
        "            a2c.update((state_buffer[0], action_buffer[0], next_state, sum([(GAMMA ** i) * r for i, r in enumerate(reward_buffer)]), done))\n",
        "        state = next_state\n",
        "        #env.render()\n",
        "    scores.append(total_reward)\n",
        "    if len(reward_buffer) == N_STEP:\n",
        "        rb = list(reward_buffer)\n",
        "        for k in range(1, N_STEP):\n",
        "            a2c.update((state_buffer[k], action_buffer[k], next_state, sum([(GAMMA ** i) * r for i, r in enumerate(rb[k:])]), done))\n",
        "\n",
        "    if (i + 1) % 75 == 0:\n",
        "        current_score = np.mean(scores)\n",
        "        print(f'Current score: {current_score}')\n",
        "        scores = []\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            a2c.save(75)\n",
        "            print(f'Best model saved with score: {best_score}')\n",
        "        end = time.time()\n",
        "        elapsed = end - start\n",
        "        start = end\n",
        "        print(f'Elapsed time: {elapsed}')\n",
        "    #elif (i + 1) % 25 == 0:\n",
        "    #    current_score_25 = np.mean(scores[-25:])\n",
        "    #    print(f'Intermediate score: {current_score_25}')\n",
        "    #    if current_score_25 > best_score_25:\n",
        "    #        best_score_25 = current_score_25\n",
        "    #        a2c.save(25)\n",
        "    #        print(f'Best 25 model saved with score: {best_score_25}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current score: -1419.348671927737\n",
            "Best model saved with score: -1419.348671927737\n",
            "Elapsed time: 23.74226450920105\n",
            "Current score: -1419.9237573494704\n",
            "Elapsed time: 23.86884880065918\n",
            "Current score: -1085.568547592379\n",
            "Best model saved with score: -1085.568547592379\n",
            "Elapsed time: 23.734857320785522\n",
            "Current score: -912.845817549795\n",
            "Best model saved with score: -912.845817549795\n",
            "Elapsed time: 23.656028270721436\n",
            "Current score: -882.2347368585605\n",
            "Best model saved with score: -882.2347368585605\n",
            "Elapsed time: 23.521143674850464\n",
            "Current score: -814.7592419184786\n",
            "Best model saved with score: -814.7592419184786\n",
            "Elapsed time: 23.611609935760498\n",
            "Current score: -692.1581229134532\n",
            "Best model saved with score: -692.1581229134532\n",
            "Elapsed time: 24.835127592086792\n",
            "Current score: -583.0617913673863\n",
            "Best model saved with score: -583.0617913673863\n",
            "Elapsed time: 24.328096628189087\n",
            "Current score: -464.2403707840897\n",
            "Best model saved with score: -464.2403707840897\n",
            "Elapsed time: 24.05731511116028\n",
            "Current score: -453.9268631641373\n",
            "Best model saved with score: -453.9268631641373\n",
            "Elapsed time: 23.986087799072266\n",
            "Current score: -375.78742129525756\n",
            "Best model saved with score: -375.78742129525756\n",
            "Elapsed time: 23.923736810684204\n",
            "Current score: -395.6601100763892\n",
            "Elapsed time: 24.221188068389893\n",
            "Current score: -361.91482432961016\n",
            "Best model saved with score: -361.91482432961016\n",
            "Elapsed time: 24.13285756111145\n",
            "Current score: -355.19227763015044\n",
            "Best model saved with score: -355.19227763015044\n",
            "Elapsed time: 24.56934666633606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtuIjQ6LB8Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}