{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_HW02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmlvwmTl8ASb6wSh/mLhMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamSaw/deep-reinforcement-learning/blob/master/RL_HW02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFcjbVWltzd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "f6c48a0b-499a-43b0-ea6b-27418d11dc3a"
      },
      "source": [
        "!pip install gym[box2d]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.17.5)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOzDrxDSt3TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "SEED = 11  # 627, 8, 11\n",
        "\n",
        "\n",
        "def make_reproducible(seed, make_cuda_reproducible):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if make_cuda_reproducible:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "make_reproducible(SEED, make_cuda_reproducible=False)\n",
        "\n",
        "\n",
        "def transform_state(state):\n",
        "    return torch.tensor(state)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.model = Agent.generate_model()\n",
        "        self.model.load_state_dict(torch.load(__file__[:-8] + \"/agent.pkl\"))\n",
        "        self.model.to(torch.device(\"cpu\"))\n",
        "        self.model.eval()\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_model():\n",
        "        linears = [nn.Linear(8, 128), nn.Linear(128, 64), nn.Linear(64, 4)]\n",
        "        #for l in linears:\n",
        "        #    nn.init.uniform_(l.weight)\n",
        "        #    nn.init.uniform_(l.bias)\n",
        "        return nn.Sequential(\n",
        "                linears[0],\n",
        "                nn.ReLU(),\n",
        "                linears[1],\n",
        "                nn.ReLU(),\n",
        "                linears[2])\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = transform_state(state)\n",
        "            return torch.argmax(self.model(state)).item()\n",
        "\n",
        "    def reset(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhC6eJYuRFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from gym import make\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "N_STEP = 1\n",
        "GAMMA = 0.99\n",
        "LR = 5e-4\n",
        "BATCH_SIZE = 64\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 0.995\n",
        "TARGET_UPDATE = 10\n",
        "MEMORY_CAPACITY = int(1e5)\n",
        "MAX_EPISODE_STEPS = 1000\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.gamma = GAMMA ** N_STEP\n",
        "        self.model = Agent.generate_model().to(DEVICE)\n",
        "        self.target = Agent.generate_model().to(DEVICE)\n",
        "        self.target.load_state_dict(self.model.state_dict())\n",
        "        self.target.eval()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)\n",
        "        self.memory = ReplayMemory(MEMORY_CAPACITY)\n",
        "\n",
        "    def optimizer_step(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                                batch.next_state)), device=DEVICE, dtype=torch.bool)\n",
        "        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
        "        state_batch = torch.stack(batch.state)\n",
        "        action_batch = torch.stack(batch.action)\n",
        "        reward_batch = torch.stack(batch.reward)\n",
        "\n",
        "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "        next_state_values = torch.zeros(BATCH_SIZE, device=DEVICE)\n",
        "        next_state_values[non_final_mask] = self.target(non_final_next_states).max(1)[0].detach()\n",
        "        expected_state_action_values = (next_state_values.unsqueeze(1) * GAMMA) + reward_batch\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.model.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update(self, transition):\n",
        "        state, action, next_state, reward, done = transition\n",
        "        self.memory.push(torch.tensor(state, device=DEVICE),\n",
        "                         torch.tensor([action], device=DEVICE),\n",
        "                         torch.tensor(next_state, device=DEVICE) if not done else None,\n",
        "                         torch.tensor([reward], device=DEVICE))\n",
        "        self.optimizer_step()\n",
        "\n",
        "    def act(self, state, target=False):\n",
        "        with torch.no_grad():\n",
        "            state = transform_state(state).to(DEVICE)\n",
        "            return torch.argmax(self.model(state)).item()\n",
        "\n",
        "    def save(self, i):\n",
        "        torch.save(self.model.state_dict(), f'agent_{i}.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-gUpArZuVIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86a41176-86a3-45a3-944e-80c046bdbc5f"
      },
      "source": [
        "env = make(\"LunarLander-v2\")\n",
        "dqn = DQN(state_dim=8, action_dim=4)\n",
        "episodes = 1000\n",
        "scores = []\n",
        "best_score = -1000.0\n",
        "best_score_10 = -1000.0\n",
        "total_steps = 0\n",
        "start = time.time()\n",
        "eps = EPS_START\n",
        "for i in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    reward_buffer = deque(maxlen=N_STEP)\n",
        "    state_buffer = deque(maxlen=N_STEP)\n",
        "    action_buffer = deque(maxlen=N_STEP)\n",
        "    while not done and steps < MAX_EPISODE_STEPS:\n",
        "        #eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * total_steps / EPS_DECAY)\n",
        "        total_steps += 1\n",
        "        if random.random() < eps:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = dqn.act(state)\n",
        "        eps = max(EPS_END, EPS_DECAY * eps)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        reward_buffer.append(reward)\n",
        "        state_buffer.append(state)\n",
        "        action_buffer.append(action)\n",
        "        if len(reward_buffer) == N_STEP:\n",
        "            dqn.update((state_buffer[0], action_buffer[0], next_state, sum([(GAMMA ** i) * r for i, r in enumerate(reward_buffer)]), done))\n",
        "        state = next_state\n",
        "        #env.render()\n",
        "    scores.append(total_reward)\n",
        "    if len(reward_buffer) == N_STEP:\n",
        "        rb = list(reward_buffer)\n",
        "        for k in range(1, N_STEP):\n",
        "            dqn.update((state_buffer[k], action_buffer[k], next_state, sum([(GAMMA ** i) * r for i, r in enumerate(rb[k:])]), done))\n",
        "\n",
        "    if (i + 1) % TARGET_UPDATE == 0:\n",
        "        dqn.target.load_state_dict(dqn.model.state_dict())\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        current_score = np.mean(scores)\n",
        "        print(f'Current score: {current_score}')\n",
        "        scores = []\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            dqn.save(50)\n",
        "            print(f'Best model saved with score: {best_score}')\n",
        "        end = time.time()\n",
        "        elapsed = end - start\n",
        "        start = end\n",
        "        print(f'Elapsed time: {elapsed}')\n",
        "    elif (i + 1) % 10 == 0:\n",
        "        current_score_10 = np.mean(scores[-10:])\n",
        "        print(f'Intermediate score: {current_score_10}')\n",
        "        if current_score_10 > best_score_10:\n",
        "          best_score_10 = current_score_10\n",
        "          dqn.save(10)\n",
        "          print(f'Best 10 model saved with score: {best_score_10}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intermediate score: -281.41897111817434\n",
            "Best 10 model saved with score: -281.41897111817434\n",
            "Intermediate score: -46.96905323592746\n",
            "Best 10 model saved with score: -46.96905323592746\n",
            "Intermediate score: -155.81274938006132\n",
            "Intermediate score: -147.36453228998081\n",
            "Current score: -141.74980542169703\n",
            "Best model saved with score: -141.74980542169703\n",
            "Elapsed time: 50.372435092926025\n",
            "Intermediate score: 2.7922781639780228\n",
            "Best 10 model saved with score: 2.7922781639780228\n",
            "Intermediate score: -50.42879359993785\n",
            "Intermediate score: -92.98412290708595\n",
            "Intermediate score: -113.35472561136089\n",
            "Current score: -69.64240826190857\n",
            "Best model saved with score: -69.64240826190857\n",
            "Elapsed time: 144.4490396976471\n",
            "Intermediate score: -131.72478998131479\n",
            "Intermediate score: -92.42112817424876\n",
            "Intermediate score: -102.78507654976973\n",
            "Intermediate score: -110.45663200333225\n",
            "Current score: -108.33441977196375\n",
            "Elapsed time: 144.95271754264832\n",
            "Intermediate score: -71.43654401238464\n",
            "Intermediate score: -55.65360771851056\n",
            "Intermediate score: -99.71530100270859\n",
            "Intermediate score: -84.7818367347464\n",
            "Current score: -72.39325908476098\n",
            "Elapsed time: 145.95228910446167\n",
            "Intermediate score: -33.039951999075214\n",
            "Intermediate score: -20.675037070077327\n",
            "Intermediate score: -58.58500560573748\n",
            "Intermediate score: 2.3156217289549206\n",
            "Current score: -30.235588288389927\n",
            "Best model saved with score: -30.235588288389927\n",
            "Elapsed time: 166.82693886756897\n",
            "Intermediate score: -39.18175565179807\n",
            "Intermediate score: -37.91568865507701\n",
            "Intermediate score: 19.387391467538112\n",
            "Best 10 model saved with score: 19.387391467538112\n",
            "Intermediate score: 22.00472497102284\n",
            "Best 10 model saved with score: 22.00472497102284\n",
            "Current score: -7.674157255971767\n",
            "Best model saved with score: -7.674157255971767\n",
            "Elapsed time: 130.88030004501343\n",
            "Intermediate score: 46.39071147673939\n",
            "Best 10 model saved with score: 46.39071147673939\n",
            "Intermediate score: 62.7620182997812\n",
            "Best 10 model saved with score: 62.7620182997812\n",
            "Intermediate score: -21.8978201272063\n",
            "Intermediate score: 59.95806272904624\n",
            "Current score: 33.77481411640184\n",
            "Best model saved with score: 33.77481411640184\n",
            "Elapsed time: 157.3215708732605\n",
            "Intermediate score: 79.57900249995328\n",
            "Best 10 model saved with score: 79.57900249995328\n",
            "Intermediate score: 79.78358288018433\n",
            "Best 10 model saved with score: 79.78358288018433\n",
            "Intermediate score: 47.88738152770422\n",
            "Intermediate score: 63.17028150476041\n",
            "Current score: 77.57678830232362\n",
            "Best model saved with score: 77.57678830232362\n",
            "Elapsed time: 164.1886727809906\n",
            "Intermediate score: 133.2359225890464\n",
            "Best 10 model saved with score: 133.2359225890464\n",
            "Intermediate score: 136.69447451177038\n",
            "Best 10 model saved with score: 136.69447451177038\n",
            "Intermediate score: 117.41248368295153\n",
            "Intermediate score: 147.71747215397653\n",
            "Best 10 model saved with score: 147.71747215397653\n",
            "Current score: 131.3036630634503\n",
            "Best model saved with score: 131.3036630634503\n",
            "Elapsed time: 152.18717336654663\n",
            "Intermediate score: 202.33665460920068\n",
            "Best 10 model saved with score: 202.33665460920068\n",
            "Intermediate score: 162.18327052416794\n",
            "Intermediate score: 144.22440049452308\n",
            "Intermediate score: 158.23112176381898\n",
            "Current score: 172.46485643901727\n",
            "Best model saved with score: 172.46485643901727\n",
            "Elapsed time: 111.19308924674988\n",
            "Intermediate score: 169.0094663318488\n",
            "Intermediate score: 181.66769411177066\n",
            "Intermediate score: 175.35324436000755\n",
            "Intermediate score: 148.1435286321624\n",
            "Current score: 164.77081143927725\n",
            "Elapsed time: 116.65340232849121\n",
            "Intermediate score: 118.69717426962424\n",
            "Intermediate score: 125.06145169032376\n",
            "Intermediate score: 184.60383002451482\n",
            "Intermediate score: 146.54339186280464\n",
            "Current score: 154.8612576321775\n",
            "Elapsed time: 114.47104668617249\n",
            "Intermediate score: 187.14996603897652\n",
            "Intermediate score: 169.01678727443758\n",
            "Intermediate score: 157.24381350149446\n",
            "Intermediate score: 204.61920028372822\n",
            "Best 10 model saved with score: 204.61920028372822\n",
            "Current score: 176.84001192282332\n",
            "Best model saved with score: 176.84001192282332\n",
            "Elapsed time: 108.81672167778015\n",
            "Intermediate score: 192.8329310804788\n",
            "Intermediate score: 159.27108337110795\n",
            "Intermediate score: 102.52054959540774\n",
            "Intermediate score: 133.33137909328113\n",
            "Current score: 165.8080876362183\n",
            "Elapsed time: 123.37125396728516\n",
            "Intermediate score: 208.8093607981601\n",
            "Best 10 model saved with score: 208.8093607981601\n",
            "Intermediate score: 233.15020757172178\n",
            "Best 10 model saved with score: 233.15020757172178\n",
            "Intermediate score: 203.68787892237907\n",
            "Intermediate score: 221.4085606697093\n",
            "Current score: 208.1370568571446\n",
            "Best model saved with score: 208.1370568571446\n",
            "Elapsed time: 98.16374397277832\n",
            "Intermediate score: 207.90903609024184\n",
            "Intermediate score: 199.85642956891084\n",
            "Intermediate score: 239.8557819465048\n",
            "Best 10 model saved with score: 239.8557819465048\n",
            "Intermediate score: 210.36440342969405\n",
            "Current score: 208.49288233122496\n",
            "Best model saved with score: 208.49288233122496\n",
            "Elapsed time: 89.46807408332825\n",
            "Intermediate score: 237.42117583090948\n",
            "Intermediate score: 176.4503212107325\n",
            "Intermediate score: 223.42406371215333\n",
            "Intermediate score: 165.79628896816106\n",
            "Current score: 188.18343461687388\n",
            "Elapsed time: 99.3070068359375\n",
            "Intermediate score: 183.41249892737792\n",
            "Intermediate score: 200.76536560307238\n",
            "Intermediate score: 208.3963556877032\n",
            "Intermediate score: 183.1971340017018\n",
            "Current score: 204.76700594267425\n",
            "Elapsed time: 111.1703872680664\n",
            "Intermediate score: 246.35177572206945\n",
            "Best 10 model saved with score: 246.35177572206945\n",
            "Intermediate score: 190.9380979271126\n",
            "Intermediate score: 224.1672451459492\n",
            "Intermediate score: 197.5592467557701\n",
            "Current score: 208.93233794134773\n",
            "Best model saved with score: 208.93233794134773\n",
            "Elapsed time: 80.36393475532532\n",
            "Intermediate score: 236.05863019836085\n",
            "Intermediate score: 252.88058747995333\n",
            "Best 10 model saved with score: 252.88058747995333\n",
            "Intermediate score: 215.2181035717086\n",
            "Intermediate score: 234.4145125103335\n",
            "Current score: 235.05924970445216\n",
            "Best model saved with score: 235.05924970445216\n",
            "Elapsed time: 75.80479311943054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYlZLbBouY9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}